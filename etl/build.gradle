buildscript {
  repositories {
    mavenCentral()
  }
  dependencies {
    classpath 'org.codehaus.groovy.modules.http-builder:http-builder:0.7'
  }
}

import groovy.json.JsonSlurper
import groovy.xml.XmlUtil
import groovyx.net.http.HTTPBuilder
import groovyx.net.http.Method

def parentIdentifier

def sources = [
    [
        name             : 'GHRSST',
        class            : GeoportalExtractor,
        collectionsUrl   : 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title:GHRSST%20NOT%20title:Documentation'],
        granulesUrl      : 'http://www.nodc.noaa.gov/search/granule/rest/find/document',
        granulesParams   : [searchText: "fileIdentifier:${-> parentIdentifier}*"]
    ],
    [
        name             : 'COOPS',
        class            : GeoportalExtractor,
        collectionsUrl   : 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title:NWLON%20NOT%20title:Documentation'],
        granulesUrl      : 'http://www.nodc.noaa.gov/search/granule/rest/find/document',
        granulesParams   : [searchText: 'fileIdentifier%3ACO-OPS*', after: '2016-06-30']
    ],
    [
        name             : 'DEM',
        class            : WafExtractor,
        collectionsUrl   : 'https://www.ngdc.noaa.gov/metadata/published/NOAA/NESDIS/NGDC/MGG/DEM/iso/xml',
        collectionsParams: [:]
    ]
]

def targets = [
    [
        name : 'LocalApi',
        class: ApiLoader,
        url  : 'http://localhost:8097/api/metadata'
    ],
    [
        name : 'CUApi',
        class: ApiLoader,
        url  : 'https://sciapps.colorado.edu/api/metadata'
    ],
    [
        name : 'LocalGeoportal',
        class: GeoportalLoader,
        url  : "http://localhost:8888/rest/metadata/item"
    ]
]

// Creates a task to load from each source into each target,
// plus a task to load all sources into each target
targets.each { target ->
  def tasksForTarget = sources.collect { source ->
    tasks.create("etl${source.name}To${target.name}").doFirst {
      // Load collections first
      def extractor = source.class.newInstance(source.collectionsUrl, source.collectionsParams)
      def loader = target.class.newInstance(extractor, target.url)
      loader.load()

      // Load granules if they exist
      if(source.granulesUrl) {
        def collections = loader.collections
        collections.each { i ->
          // ParentID is only relevant to GHRSST but set here to avoid duplicate code below since granulesParams
          // is dynamically constructed for each collection
          parentIdentifier = i
          extractor = source.class.newInstance(source.granulesUrl, source.granulesParams)
          loader = target.class.newInstance(extractor, target.url)
          if (source.name == 'GHRSST') {
            loader.load(parentIdentifier)
          } else {
            loader.load()
          }
        }
      }
    }
  }
  tasks.create("etlAllTo${target.name}").dependsOn(tasksForTarget)
}


class WafExtractor implements Iterator<String> {
  private String endpointUrl
  private Map searchParams
  private Iterator wafIterator

  WafExtractor(String endpointUrl, Map searchParams = [:]) {
    this.endpointUrl = endpointUrl
    this.searchParams = searchParams

    def html = new HTTPBuilder(endpointUrl).get(searchParams)
    def xmlFiles = html.'**'.collect({ "${endpointUrl}/${it}" }).findAll({ it.endsWith('.xml') })
    this.wafIterator = xmlFiles.iterator()
  }

  @Override
  boolean hasNext() {
    return wafIterator.hasNext()
  }

  @Override
  String next() {
    return wafIterator.next()
  }
}


class GeoportalExtractor implements Iterator<String> {
  private String endpointUrl
  private Map searchParams
  private Integer pageSize = 1000
  private List currentPage = []
  private Integer currentPageNum = 0
  private Integer currentIndex = 0
  private JsonSlurper jsonSlurper = new JsonSlurper()

  GeoportalExtractor(String endpointUrl, Map searchParams = [:]) {
    this.endpointUrl = endpointUrl
    this.searchParams = searchParams
    getNextPage()
  }

  @Override
  boolean hasNext() {
    if (currentIndex < currentPage.size()) {
      return true
    } else {
      getNextPage()
      return currentPage.size() > 0
    }
  }

  @Override
  String next() {
    return hasNext() ? currentPage[currentIndex++] : null
  }

  private getNextPage() {
    currentPage = jsonSlurper.parse(buildUrl()).records.collect { record ->
      record.links.find({ it.type.contains('metadata') }).href
    }
    currentIndex = 0
    currentPageNum++
  }

  private buildUrl() {
    def params = searchParams + [start: currentPageNum * pageSize, max: pageSize, f: 'json', orderBy: 'title']
    def queryString = params.collect({ k, v -> k + '=' + v }).join('&')
    def url = endpointUrl + '?' + queryString
    println url
    return new URL(url)
  }
}

class ApiLoader {
  protected Iterator<String> metadataUrls
  protected HTTPBuilder target
  protected Method httpMethod

  private collections = []

  ApiLoader(Iterator<String> metadataUrls, String apiEndpoint) {
    this.metadataUrls = metadataUrls
    this.target = new HTTPBuilder(apiEndpoint)
    this.target.ignoreSSLIssues()
    this.httpMethod = Method.POST
  }

  void load() {
    def successes = 0
    def failures = 0
    metadataUrls.eachWithIndex { metadata, i ->
      try {
        def metadataContent = new URL(metadata).text
        target.request(httpMethod) {
          requestContentType = 'application/xml'
          body = metadataContent
          response.success = { resp, data ->
            successes++
            println "${data.data.id} created #${i}: ${data.data.attributes.created}"
            collections.add(data.data.id.replace('gov.noaa.nodc:', ''))
          }
          response.failure = { resp, data ->
            failures++
            println "Error uploading metadata #${i} from ${metadata}: ${data}"
          }
        }
      }
      catch (e) {
        println "Error downloading metadata #${i} from ${metadata}: ${e.message}"
        return // break out of current each{} loop
      }
    }
    println "Load complete. Successfully loaded ${successes} of ${successes + failures} metadata records"
  }

  void load(String parentIdentifier) {
    def successes = 0
    def failures = 0

    parentIdentifier = 'gov.noaa.nodc:' + parentIdentifier
    def xmlFragment = """  <gmd:parentIdentifier>
    <gco:CharacterString>${parentIdentifier}</gco:CharacterString>
  </gmd:parentIdentifier>"""
    def fragment = new XmlParser(false, false).parseText(xmlFragment)

    metadataUrls.eachWithIndex { metadata, i ->
      try {
        def root = new XmlParser(false, false).parseText(new URL(metadata).text)
        def xml = root.children()
        xml.add(fragment)
        def updatedXml = XmlUtil.serialize(root)
          target.request(httpMethod) {
            requestContentType = 'application/xml'
            body = updatedXml
            response.success = { resp, data ->
              successes++
              println "${data.data.id} created #${i}: ${data.data.attributes.created}"
            }
            response.failure = { resp, data ->
              failures++
              println "Error uploading metadata #${i} from ${metadata}: ${data}"
            }
          }
      }
      catch(e) {
        println "Error downloading metadata #${i} from ${metadata}: ${e.message}"
        return // break out of current each{} loop
      }
    }

    println "Load complete. Successfully loaded ${successes} of ${successes + failures} metadata records"
  }

  List getCollections() {
    return collections
  }
}

class GeoportalLoader extends ApiLoader {
  GeoportalLoader(Iterator<String> metadataUrls, String geoportalEndpoint) {
    super(metadataUrls, geoportalEndpoint)
    this.target.auth.basic('gptadmin', 'gptadmin')
    this.httpMethod = Method.PUT
  }
}
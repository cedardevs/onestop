import groovy.json.JsonSlurper
import groovy.xml.XmlUtil
import groovyx.net.http.HTTPBuilder
import groovyx.net.http.Method

buildscript {
  repositories {
    mavenCentral()
  }
  dependencies {
    classpath 'org.codehaus.groovy.modules.http-builder:http-builder:0.7'
  }
}

def sources = [
    [
        name             : 'GptCollections',
        class            : GeoportalExtractor,
        collectionsUrl   : 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title%3A*%20NOT%20title%3AGHRSST'], //All collections not part of GHRSST
        requireBrowseImg : true,
        granules         : false
    ],
    [
        name             : 'GHRSST',
        class            : GeoportalExtractor,
        collectionsUrl   : 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title:GHRSST%20NOT%20title:Documentation'],
        requireBrowseImg : false,
        granules         : [
          url      : 'http://www.nodc.noaa.gov/search/granule/rest/find/document',
          params   : [searchText: "fileIdentifier:REPLACE_PARAMS*"]
        ]
    ],
    [
        name             : 'COOPS',
        class            : GeoportalExtractor,
        collectionsUrl   : 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title:NWLON%20NOT%20title:Documentation'],
        requireBrowseImg : false,
        granules         : [
          url      : 'http://www.nodc.noaa.gov/search/granule/rest/find/document',
          params   : [searchText: 'fileIdentifier%3ACO-OPS*', after: '2016-06-30']
        ]
    ],
    [
        name             : 'DEM',
        class            : WafExtractor,
        collectionsUrl   : 'https://ngdc.noaa.gov/metadata/published/NOAA/OneStop/DEM/iso/xml',
        collectionsParams: [:],
        requireBrowseImg : false,
        granules         : false
    ],
    [
        name: 'SAMOS',
        class: GeoportalExtractor,
        collectionsUrl: 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title:SAMOS'],
        requireBrowseImg : false,
        granules         : [
          url: 'http://data.nodc.noaa.gov/nodc/archive/metadata/test/granule/iso/samos/',
          params: [:],
          class: WafExtractor,
        ]
    ],
    [
        name: 'WOA13',
        class: GeoportalExtractor,
        collectionsUrl: 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'fileIdentifier:"gov.noaa.nodc:0114815"'],
        requireBrowseImg : false,
        granules         : [
          url: 'http://data.nodc.noaa.gov/nodc/archive/metadata/test/granule/iso/woa13/',
          params: [:],
          class: WafExtractor,
        ]
    ]
]

def targets = [
    [
        name : 'LocalApi',
        class: Loader,
        url  : 'http://localhost:8097/onestop/api/metadata'
    ],
    [
        name : 'CUApi',
        class: Loader,
        url  : 'https://sciapps.colorado.edu/onestop/api/metadata'
    ],
    [
        name : 'LocalGeoportal',
        class: GeoportalLoader,
        url  : "http://localhost:8888/rest/metadata/item"
    ],
    [
        name : 'LocalDisk',
        class: Loader,
        filePath  : "/tmp"
    ]
]

// Return a closure resource for loading sources into each target
def loadToTarget( Map source, Map target ) {
  return {
    // Load collections first
    def extractor = source.class.newInstance(source.collectionsUrl, source.collectionsParams)
    def loader, granuleLoader
    if (target.filePath){
      loader = target.class.newInstance(extractor, target.filePath, source.name)
    } else {
      loader = target.class.newInstance(extractor, target.url)
    }
    loader.load(source.requireBrowseImg)

    // Load granules if they exist
    def granulesAttrs = source.granules
    if (granulesAttrs) {
      def collections = loader.collections
      def isCollection = false
      collections.each { parentId ->
        if (granulesAttrs.class) {
          extractor = granulesAttrs.class.newInstance(granulesAttrs.url, granulesAttrs.params, parentId)
        } else {
          extractor = source.class.newInstance(granulesAttrs.url, granulesAttrs.params, parentId)
        }
        if (target.filePath){
          granuleLoader = target.class.newInstance(extractor, target.filePath, source.name)
        } else {
          granuleLoader = target.class.newInstance(extractor, target.url)
        }
        // ParentID only relevant for GHRSST
        granuleLoader.load(source.requireBrowseImg, isCollection,
          source.name == 'GHRSST' ? parentId : null)
      }
    }
  }
}

// Creates a task to load from each source into local & server targets
targets.each { target ->
  tasks.create("etlAllTo${target.name}")
    .dependsOn( sources.collect { source ->
      tasks.create("etl${source.name}To${target.name}")
        .doFirst(loadToTarget(source, target))
  })
}

class WafExtractor implements Iterator<String> {
  private String endpointUrl
  private Map searchParams
  private Iterator wafIterator

  WafExtractor(String endpointUrl, Map searchParams = [:], String parentId = null) {
    this.endpointUrl = endpointUrl
    this.searchParams = searchParams

    def html = new HTTPBuilder(endpointUrl).get(searchParams)
    def xmlFiles = html.'**'.collect({ "${endpointUrl}/${it}" }).findAll({ it.endsWith('.xml') })
    this.wafIterator = xmlFiles.iterator()
  }

  @Override
  boolean hasNext() {
    return wafIterator.hasNext()
  }

  @Override
  String next() {
    return wafIterator.next()
  }
}


class GeoportalExtractor implements Iterator<String> {
  private final def GPT_PAGE_REQUESTS = 0
  private final def MAX_GPT_PAGE_REQUESTS = 2000 //TODO Should be an arg passed in; Update me as needed!
  private String endpointUrl
  private Map searchParams
  private Integer pageSize = 250
  private List currentPage = []
  private Integer currentPageNum = 0
  private Integer currentIndex = 0
  private JsonSlurper jsonSlurper = new JsonSlurper()
  def parentId

  GeoportalExtractor(String endpointUrl, Map searchParams = [:], String parentId = null) {
    this.endpointUrl = endpointUrl
    this.searchParams = searchParams
    this.parentId = parentId
    getNextPage()
  }

  @Override
  boolean hasNext() {
    if (currentIndex < currentPage.size()) {
      return true
    } else {
      GPT_PAGE_REQUESTS++
      println "Number of gptPageRequests: ${GPT_PAGE_REQUESTS}"
      if (GPT_PAGE_REQUESTS < MAX_GPT_PAGE_REQUESTS) {
        getNextPage()
        return currentPage.size() > 0
      } else {
        return false
      }
    }
  }

  @Override
  String next() {
    return hasNext() ? currentPage[currentIndex++] : null
  }

  private getNextPage() {
    try {
      currentPage = jsonSlurper.parse(buildUrl()).records.collect { record ->
        record.links.find({ it.type.contains('metadata') }).href
      }
    } catch (e) {
      println "getNextPage() failed. Go to next page..."
      sleep(3000)
    }
    currentIndex = 0
    currentPageNum++
  }

  private buildUrl() {
    if (parentId) {
      searchParams = searchParams.collectEntries { k, v ->
        [k, v.replaceAll('REPLACE_PARAMS', parentId)]
      }
    }
    def params = searchParams + [start: currentPageNum * pageSize, max: pageSize, f: 'json', orderBy: 'title']
    def queryString = params.collect({ k, v -> k + '=' + v }).join('&')
    def url = endpointUrl + '?' + queryString
    println url
    return new URL(url)
  }
}

class Loader {
  final def GRANULE_LIMIT = 500
  protected Iterator<String> metadataUrls
  protected Method httpMethod = Method.POST
  def xmlParser = new XmlParser(false, false)
  def xmlSlurper, target
  def successes = 0
  def failures = 0
  def collections = []

  Loader(Iterator<String> metadataUrls, String targetPath, String sourceName = "") {
    this.metadataUrls = metadataUrls
    xmlSlurper = new XmlSlurper()
    xmlSlurper.setFeature("http://apache.org/xml/features/disallow-doctype-decl", false)
    if (sourceName) {
      (target = new File("$targetPath/$sourceName")).mkdirs()
    } else {
      target = new HTTPBuilder(targetPath)
    }
  }

  void load(boolean requireBrowseImg, boolean isCollection = true, String parentId = null) {
    if (target in File) {
      def fileDirName = isCollection ? "collections" : "granules"
      (target = new File((File)target, fileDirName)).mkdirs()
    }
    metadataUrls.eachWithIndex { metadata, i ->
      if (!isCollection ? i < GRANULE_LIMIT : true) {
        try {
          writeToDestination(metadata, requireBrowseImg, isCollection, parentId)
        } catch (e) {
          println "Error downloading metadata #${i} from ${metadata}: ${e.message}"
          return // break out of current loop
        }
      }
    }
  }

  private void writeToDestination(metadata, requireBrowseImg, isCollection, parentId) {
    def metadataContent
    if (parentId) { // Add parentId for GHRSST
      metadataContent = xmlParser.parseText(new URL(metadata).text)
      metadataContent.children().add(
        xmlParser.parseText(
          """<gmd:parentIdentifier>
               <gco:CharacterString>gov.noaa.nodc:
                 ${parentId}
               </gco:CharacterString>
             </gmd:parentIdentifier>"""))
      metadataContent = XmlUtil.serialize(metadataContent)
    } else {
      metadataContent = new URL(metadata).text
    }
    def metadataMap = xmlSlurper.parseText(metadataContent)
    def fileName = fileName(metadataMap)
    if (!requireBrowseImg || hasImage(metadataMap)) {
      if (target in File) {
        try {
          new File(target, "/${fileName}.xml") << metadataContent
          if (isCollection) {collections.add(fileName.tokenize(':')[1])}
          successes++
        } catch (Exception ex) {
          println "Failed to write to file $fileName, exception: $ex"
          failures++
        }
      } else {
        sendToApi(metadataContent, isCollection)
      }
      println "${successes} of ${successes + failures}, $fileName completed"
    }
  }

  private sendToApi(dataContent, isCollection) {
    target.request(httpMethod) {
      requestContentType = 'application/xml'
      body = dataContent
      response.success = { resp, data ->
        successes++
        println "${data.data.id} created: ${data.data.attributes.created}"
        if (isCollection) {collections.add(data.data.id.replace('gov.noaa.nodc:', ''))}
      }
      response.failure = { resp, data ->
        failures++
        println "Error uploading metadata: ${data}"
      }
    }
  }

  private boolean hasImage(metadataMap) {
    def idInfo = metadataMap.identificationInfo.MD_DataIdentification
    def thumbnail = idInfo.graphicOverview.MD_BrowseGraphic.fileName.CharacterString.text()
    return thumbnail?.trim()
  }

  private String fileName(metadataMap) {
    def filename = metadataMap.fileIdentifier.CharacterString.toString()
    filename = filename.replaceAll('/', '-')
    return filename
  }
}

class GeoportalLoader extends Loader {
  GeoportalLoader(Iterator<String> metadataUrls, String geoportalEndpoint) {
    super(metadataUrls, new HTTPBuilder(geoportalEndpoint))
    this.target.auth.basic('gptadmin', 'gptadmin')
    this.httpMethod = Method.PUT
  }
}
